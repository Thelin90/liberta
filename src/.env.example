# .env

# PostgreSQL
DB_NAME = metabase
DB_PASSWORD = password
DB_USER = metabase
SCHEMA = analysis
DIMENSION_USER_TABLE = d_user
DIMENSION_SESSION_TABLE = d_session
FACT_REVENUE_TABLE = f_revenue

# jars
POSTGRESQL_JAR = postgresql-42.2.7.jar

# pyspark packages
POSTGRES = org.postgresql.postgresql:42.2.6
DELTA_LOGSTORE = spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore
DELTA = io.delta:delta-core_2.11:0.3.0
HADOOP_COMMON = org.apache.hadoop:hadoop-common:2.7.7
HADOOP_AWS = org.apache.hadoop:hadoop-aws:2.7.7
PYSPARK_SUBMIT_ARGS = ${HADOOP_AWS},${HADOOP_COMMON},${DELTA}
PYSPARK_CONF_ARGS = ${DELTA_LOGSTORE}
PYSPARK_JARS_ARGS = --driver-class-path ${POSTGRESQL_JAR}

# raw 2 pg
RAW_TO_WAREHOUSE_LOCATION=r2p-spark-warehouse
WRITE_TO_PG_APP=WRITE_TO_PG
RAW_DATA_S3_BUCKET=rawevents

# AWS
AWS_ENDPOINT_URL=http://127.0.0.1:9000
AWS_ACCESS_KEY_ID=libertapoweruser
AWS_SECRET_ACCESS_KEY=libertapoweruserpassword
SIGNATURE_VERSION=s3v4